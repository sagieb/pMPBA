{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7f069c3-3035-4956-ba20-4adec8e3fb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/labs/barkailab/offirlu/git_mpba/pMPBA/res_files'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "curr_dir = os.getcwd()\n",
    "res_folder_path = os.path.abspath(os.path.join(curr_dir,'../res_files'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cbbe3-585e-4829-b39a-8367e3fc1ba7",
   "metadata": {},
   "source": [
    "<font size=\"12\">Functions</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7edba26c-b141-4a94-a1c3-bc0ecfc7c723",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(curr_dir, 'data')\n",
    "\n",
    "\n",
    "def normTP0(df):\n",
    "    '''This function normlizes to TP0'''\n",
    "    fin_df = pd.DataFrame()\n",
    "    bio_rep = list(map(lambda x: x.split('_')[0],df.columns.values)) #gets the bio rep of each column\n",
    "    for br in np.unique(bio_rep):\n",
    "        sub_df = df.iloc[:,[i for i,x in enumerate(bio_rep) if br==x]] #create a temp df of current bio reps columns\n",
    "        zero_tp_loc = sub_df.columns.str.contains('_0') #get tp0\n",
    "        sub_df = sub_df.sub(sub_df.loc[:,zero_tp_loc].values) #norm to tp0\n",
    "        sub_df = sub_df.drop(sub_df.columns[np.where(zero_tp_loc)[0][0]],axis=1) #remove tp0\n",
    "        fin_df = pd.concat([fin_df,sub_df],axis=1)\n",
    "    return fin_df\n",
    "\n",
    "def normReads(fileName,meta_df):\n",
    "    '''This function normalize reads and remove bad samples'''\n",
    "    #load experiment and normalize to num of reads\n",
    "    folder_path = os.path.abspath(os.path.join(curr_dir,'../res_files'))\n",
    "    df = pd.read_csv(os.path.join(folder_path, fileName),index_col=0)\n",
    "    df_norm = np.log2((df.div(df.sum()) * 1000000) + 1)  \n",
    "    \n",
    "    #load sampleInfo.csv,remove bad samples, average repeats\n",
    "    badSamples = meta_df.loc[meta_df[\"res_file_name\"]==fileName][\"bad_samples\"].values[0]\n",
    "    \n",
    "    if badSamples == 'non':\n",
    "        tp_index = df_norm.columns.str.split('_').str[0] + '_' + df_norm.columns.str.split('_').str[1]\n",
    "        df_norm = df_norm.groupby(tp_index, axis=1).mean()\n",
    "    elif badSamples == 'all':        \n",
    "        print('bad experiment')         \n",
    "    else:\n",
    "        badSamples = [int(i) -1 for i in badSamples.split(',')] \n",
    "        df_norm = df_norm.drop(df_norm.iloc[:, badSamples],axis = 1)\n",
    "        tp_index = df_norm.columns.str.split('_').str[0] + '_' + df_norm.columns.str.split('_').str[1]\n",
    "        print('droping samples'+ str(badSamples))\n",
    "        df_norm = df_norm.groupby(tp_index, axis=1).mean()\n",
    "        \n",
    "    return df_norm\n",
    "\n",
    "def cal_GFP(df,lib_type,minNumReads,maxSTD):\n",
    "    '''Load GFP data and calculate the mean GFP levels of each peptide'''\n",
    "    #load FACS gating data\n",
    "    if lib_type == 'Msn2T':                         \n",
    "        df_gfp = pd.read_csv(os.path.abspath(os.path.join(curr_dir,'../res_files',('BY_TEF1_'+lib_type+'_GFP_HSP26_15.csv'))),index_col=0)\n",
    "    elif lib_type == 'Pep3':\n",
    "        df_gfp = pd.read_csv(os.path.abspath(os.path.join(curr_dir,'../res_files','Pep3_GFP_HSP26.csv')),index_col=0)\n",
    "        dup_indices = [i for i, index in enumerate(df_gfp.index) if index == 'pep3']\n",
    "        for i, idx in enumerate(dup_indices):\n",
    "                df_gfp.index.values[idx] =  df_gfp.index.values[idx]+'_wt_'+str(i)\n",
    "    else:\n",
    "        df_gfp = pd.read_csv(os.path.abspath(os.path.join(curr_dir,'../res_files',('BY_TEF1_'+lib_type[0]+'_GFP_HSP26_16.csv'))),index_col=0)\n",
    "    \n",
    "    #load GFP sorting statistics\n",
    "    GFP_stats = pd.read_csv(os.path.join(curr_dir,'csv_files','FACS_stats_allLibs.csv'),index_col='library')\n",
    "    meanGFP = pd.DataFrame(GFP_stats.groupby('Population')['GFP-A Mean'].mean().sort_values(ascending=False))\n",
    "    GFP_stats = GFP_stats.filter(regex=lib_type,axis=0)\n",
    "\n",
    "    #remove columns of GFP data\n",
    "    df = df.loc[:, ~df.columns.str.contains('GFP')]\n",
    "    \n",
    "    #load GFP sequencing data, normalize, average technical repeats\n",
    "    df_gfp.index = df_gfp.index.str.replace('GAL11', 'MED15')\n",
    "    df_gfp_norm = (df_gfp.div(df_gfp.sum()) * 1000000 )+1  #normalize to read count\n",
    "    df_gfp_norm.index = df_gfp_norm.index.str.upper()\n",
    "    df_gfp_norm[np.log2(df_gfp_norm)<minNumReads] = 0    #remove cells with low amount of reads\n",
    "    \n",
    "    #calculate GFP for each repeat seperately\n",
    "    GFP_df = pd.DataFrame(index=df.index) \n",
    "\n",
    "    for rep in sorted(set(df_gfp.columns.str.split('_').str[2])):\n",
    "        rep_data = df_gfp_norm[[col for col in df_gfp_norm.columns if col.endswith('_'+ rep)]]\n",
    "        rep_data = rep_data.div(rep_data.sum(axis=1), axis=0)\n",
    "        rep_data = rep_data * meanGFP['GFP-A Mean'].values * (1- GFP_stats['%Parent'].values/100)\n",
    "        GFP_df['GFP_rep'+rep] = rep_data.sum(axis=1)\n",
    "    for index, row in GFP_df.iterrows():\n",
    "        if (row == 0).any(): \n",
    "            GFP_df.loc[index] = np.nan\n",
    "    GFP_df.loc[np.std(GFP_df,axis=1) > maxSTD] = np.nan\n",
    "    df = pd.concat([df,GFP_df],axis=1)     \n",
    "    df['GFP_mean'] = GFP_df.mean(axis=1)\n",
    "    df['GFPstd'] = np.std(GFP_df,axis=1)\n",
    "    df['GFPsem'] = np.std(GFP_df,axis=1) / np.sqrt(GFP_df.shape[1])\n",
    "    df['GFP_CV'] = np.std(GFP_df, axis=1) / np.mean(GFP_df,axis=1) * 100 \n",
    "    return df, GFP_df, df_gfp_norm\n",
    "\n",
    "\n",
    "def cal_dis_from_fit(df,parm1,parm2):\n",
    "    '''calulate distance of each point from linear regression'''\n",
    "    df_noNA = df[[parm1, parm2]].copy().dropna()\n",
    "    x = df_noNA[parm1].values[:,np.newaxis]\n",
    "    y = df_noNA[parm2]\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    df_noNA['distanceFromFit']  = y - model.predict(x)\n",
    "    df.loc[df_noNA.index,'distanceFromFit'] = df_noNA['distanceFromFit'] \n",
    "    slope = model.coef_[0]\n",
    "    return df, model\n",
    "\n",
    "def rotate_axis(df,parm1,parm2):\n",
    "    '''Perform centroid rotation acordding to the regression slope'''\n",
    "    \n",
    "    _, model_reg  = cal_dis_from_fit(df,parm1,parm2) #calculate regression\n",
    "    # Calculate centroid of the data\n",
    "    x = df[parm1]\n",
    "    y = df[parm2]\n",
    "    centroid_x = np.mean(x)\n",
    "    centroid_y = np.mean(y)\n",
    "\n",
    "    angle = model_reg.coef_ * -1 # Calculate angle of rotation to make the regression line horizontal\n",
    "    \n",
    "    # Rotate the data points around the centroid\n",
    "    #df[parm1+'_rotated'] = (x - centroid_x) * np.cos(angle) - (y - centroid_y) * np.sin(angle) + centroid_x\n",
    "    df[parm2+'_rotated'] = (x - centroid_x) * np.sin(angle) + (y - centroid_y) * np.cos(angle) + centroid_y\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5a953-1af8-4245-9452-e98a0c5517b0",
   "metadata": {},
   "source": [
    "<font size=\"6\">Domain library</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42a720b0-400d-47e2-96c0-00cde7685c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load domain raw data, normalize, add GFP data and normalize to it\n",
    "meta_df = pd.read_csv(os.path.join(curr_dir,'csv_files/file_list.csv'),index_col =0)\n",
    "csv_files = sorted(os.listdir(res_folder_path), key=str.casefold)\n",
    "\n",
    "tp = '360'\n",
    "DomainDesc = pd.read_csv(os.path.join(curr_dir,'csv_files/Domain_seqs.csv')).set_index('Pep_name')\n",
    "domain_csv = [x for x in csv_files if 'BY_TEF1_D' in x]\n",
    "domain_csv = [item for item in domain_csv if \"GFP\" not in item and \"promUpstream\" not in item] #remove GFP and promUpstream samples\n",
    "\n",
    "domain_df = pd.DataFrame()\n",
    "\n",
    "for x in domain_csv:   \n",
    "    df = normReads(x, meta_df)\n",
    "    df = normTP0(df)\n",
    "    df = df.add_prefix(x[:-4].split('_')[4] + '_' + x[:-4].split('_')[5] + '_')\n",
    "    domain_df = pd.concat([domain_df,df],axis=1)\n",
    "    \n",
    "domain_df = domain_df.filter(regex=tp)\n",
    "result = domain_df.groupby(domain_df.columns.str.split('_').str[2], axis=1).median()\n",
    "\n",
    "DomainDesc['median_FC_rep1'] = result['1']\n",
    "DomainDesc['median_FC_rep2'] = result['2']\n",
    "DomainDesc['median_FC'] = DomainDesc[['median_FC_rep1','median_FC_rep2']].median(axis=1)\n",
    "\n",
    "DomainDesc, gfp_df, df_gfp_norm = cal_GFP(DomainDesc,'D',4,500)\n",
    "DomainDesc, reg = cal_dis_from_fit(DomainDesc,'GFP_mean','median_FC')\n",
    "DomainDesc = rotate_axis(DomainDesc,'GFP_mean','median_FC')\n",
    "DomainDesc.to_csv(os.path.join(curr_dir,'supp_tables/Table_S1_Domains.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23aa30-506e-4487-acc5-faa4bc115c9d",
   "metadata": {},
   "source": [
    "<font size=\"6\">Tiling library</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d29b540-cdd9-49ca-9fe7-d9a251905f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droping samples[0]\n",
      "droping samples[4, 6]\n",
      "droping samples[5]\n",
      "droping samples[2]\n",
      "droping samples[1, 4]\n"
     ]
    }
   ],
   "source": [
    "#load Tiling raw data, normalize, add GFP data and normalize to it\n",
    "meta_df = pd.read_csv(os.path.join(curr_dir,'csv_files/file_list.csv'),index_col =0)\n",
    "csv_files = sorted(os.listdir(res_folder_path), key=str.casefold)\n",
    "\n",
    "tiling_seqs = pd.read_csv(os.path.join(curr_dir,'csv_files/Tiling_seqs.csv'),index_col=0)\n",
    "tiling_seqs['pep_middle'] =  tiling_seqs['pep_end'] - ((tiling_seqs['pep_end'] - tiling_seqs['pep_start'] +1)/2 )\n",
    "tiling_csv = [x for x in csv_files if 'BY_TEF1_T_' in x]\n",
    "tiling_csv = [item for item in tiling_csv if \"GFP\" not in item and \"promUpstream\" not in item and \"YAP6\" not in item]\n",
    "\n",
    "tiling_df = pd.DataFrame()\n",
    "\n",
    "for x in tiling_csv: \n",
    "    df = normReads(x, meta_df)\n",
    "    df = normTP0(df)\n",
    "    df = df.add_prefix(x[:-4].split('_')[4] + '_' + x[:-4].split('_')[5] + '_')\n",
    "    tiling_df = pd.concat([tiling_df,df],axis=1)\n",
    "tiling_df = tiling_df.filter(regex=tp)\n",
    "tiling_df.index = tiling_df.index.str.replace('GAL11', 'MED15')\n",
    "result = tiling_df.groupby(tiling_df.columns.str.split('_').str[2], axis=1).median()\n",
    "\n",
    "tiling_seqs['median_FC_rep1'] = result['1']\n",
    "tiling_seqs['median_FC_rep2'] = result['2']\n",
    "tiling_seqs['median_FC'] = tiling_seqs[['median_FC_rep1','median_FC_rep2']].median(axis=1)\n",
    "tiling_seqs, GFP_df, df_gfp_norm = cal_GFP(tiling_seqs,'Tiling',4,1000)\n",
    "tiling_seqs, reg = cal_dis_from_fit(tiling_seqs,'GFP_mean','median_FC')\n",
    "tiling_seqs = rotate_axis(tiling_seqs,'GFP_mean','median_FC')\n",
    "tiling_seqs.to_csv(os.path.join(curr_dir,'supp_tables/Table_S2_Tiling.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0eac3-3edc-4813-94e8-2bbd9c977d3c",
   "metadata": {},
   "source": [
    "<font size=\"6\">Msn2 tiling library</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3e066de-a081-4f31-960c-18e338eb22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droping samples[1]\n",
      "droping samples[1]\n",
      "droping samples[1]\n",
      "droping samples[1]\n"
     ]
    }
   ],
   "source": [
    "Msn2_seqs = pd.read_csv(os.path.join(curr_dir,'csv_files/Msn2Tiling_seqs.csv'),index_col=0)\n",
    "\n",
    "rel_files = [file for file in csv_files if '_'.join(['BY','TEF1','Msn2']) in file]\n",
    "rel_files = [item for item in rel_files if \"GFP\" not in item and \"promUpstream\" not in item]\n",
    "\n",
    "\n",
    "Msn2T_df = pd.DataFrame()\n",
    "for curr_file in rel_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, curr_file),index_col=0)\n",
    "    df = normReads(curr_file,meta_df)\n",
    "    df = normTP0(df)\n",
    "    df.index = df.index.str.upper()\n",
    "    Msn2T_df = pd.concat([Msn2T_df,df],axis=1)\n",
    "Msn2_seqs.index =  Msn2T_df.index     \n",
    "Msn2_seqs['median_FC'] = Msn2T_df.filter(regex=tp).median(axis=1)\n",
    "Msn2_seqs, GFP_df, gfp_bins = cal_GFP(Msn2_seqs,'Msn2T',4,1000)\n",
    "Msn2_seqs,_ = cal_dis_from_fit(Msn2_seqs,\"GFP_mean\",\"median_FC\")\n",
    "Msn2_seqs = rotate_axis(Msn2_seqs,'GFP_mean','median_FC')\n",
    "Msn2_seqs.to_csv(os.path.join(curr_dir,'supp_tables/Table_S3_Msn2Tiling.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fd95b-7fae-4f99-bcea-9237661d8c62",
   "metadata": {},
   "source": [
    "<font size=\"6\">Msn2 209:269 mutations (Pep3)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b08372f8-68a0-4cd0-9b6d-7542152766d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = pd.read_csv(os.path.join(curr_dir,'csv_files/Pep3_info.csv'),index_col='Seq_name')\n",
    "pep3_info_table.index = info_table.index.str.upper()\n",
    "\n",
    "dup_indices = [i for i, index in enumerate(pep3_info_table.index) if index == 'PEP3']\n",
    "for i, idx in enumerate(dup_indices):\n",
    "        pep3_info_table.index.values[idx] =  pep3_info_table.index.values[idx]+'_WT_'+str(i)\n",
    "        \n",
    "pep3_info_table, GFP_df, df_gfp_norm = cal_GFP(pep3_info_table,'Pep3',4,1000)\n",
    "\n",
    "#load experimental pMBPA data and normalize\n",
    "pep3_df = pd.read_csv(os.path.join(res_folder_path,'Pep3_HSP12.csv'),index_col=0)\n",
    "pep3_df.index = pep3_df.index.str.upper()\n",
    "pep3_df = np.log2((pep3_df.div(pep3_df.sum()) * 1000000) + 1)\n",
    "pep3_df = pep3_df.drop(columns=['1_0_2']) # remove one bad sample\n",
    "tp_index = pep3_df.columns.str.split('_').str[0] + '_' + pep3_df.columns.str.split('_').str[1]\n",
    "pep3_df = pep3_df.groupby(tp_index, axis=1).median()\n",
    "pep3_df = normTP0(pep3_df)\n",
    "pep3_info_table['median_360_FC'] = pep3_df.filter(regex=tp).median(axis=1).values\n",
    "\n",
    "pep3_info_table['pep_type'] = pep3_info_table.index.to_series().apply(lambda idx: 'control' if 'HXK1' in idx or 'GSY1' in idx or 'TDH3' in idx else ('WT' if 'WT' in idx else 'mutation'))\n",
    "pep3_info_table,_ = cal_dis_from_fit(pep3_info_table,'GFP_mean','median_360_FC')\n",
    "pep3_info_table = rotate_axis(pep3_info_table,'GFP_mean','median_360_FC')\n",
    "pep3_info_table['DFF_deltaFromWT'] = pep3_info_table['distanceFromFit'] - pep3_info_table[pep3_info_table['pep_type'] == 'WT']['distanceFromFit'].median()\n",
    "motif = 'IDSMLDDYVSS'\n",
    "pep3_info_table['motB'] = pep3_info_table['protein_seq'].apply(lambda x: motif in x).values\n",
    "\n",
    "pep3_info_table.index = info_table.index\n",
    "dup_indices = [i for i, index in enumerate(pep3_info_table.index) if index == 'pep3']\n",
    "for i, idx in enumerate(dup_indices):\n",
    "        pep3_info_table.index.values[idx] =  pep3_info_table.index.values[idx]+'_wt_'+str(i)\n",
    "        \n",
    "pep3_info_table.to_csv(os.path.join(curr_dir,'supp_tables/Table_S4_Msn2mutations.csv'))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peplibs2",
   "language": "python",
   "name": "peplibs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
